{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import time\n",
    "import re\n",
    "from itertools import islice\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reliable sources\n",
    "fpath_nyt = \"nyt.txt\"\n",
    "fpath_wsj = \"wsj.txt\"\n",
    "fpath_bbc = \"bbc.txt\"\n",
    "\n",
    "#Unreliable sources\n",
    "#Need more\n",
    "fpath_bb = \"bb.txt\"\n",
    "\n",
    "#ON HOLD, POTENTIALLY A STRETCH GOAL\n",
    "#Satire sources\n",
    "#fpath_onion = \"theonionlinks.txt\"\n",
    "tp = {\"wsj\", \"nyt\", \"bbc\", \"bb\", \"an\"}\n",
    "articles = {\"wsj\": ['wsj.txt', 'wsj.csv'], \"nyt\": ['nyt.txt', 'nyt.csv'],\n",
    "            \"bbc\": ['bbc.txt', 'bbc.csv'], \"bb\": ['bb.txt', 'bb.csv'],\n",
    "           \"an\": ['an.txt', 'an.csv']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Scrape(name):\n",
    "    with open(articles[name][1], 'wb') as result:\n",
    "        if name == \"wsj\":\n",
    "            writer = csv.writer(result, dialect='excel')\n",
    "            driver = webdriver.Firefox()\n",
    "            with open(articles[name][0], 'r', encoding=\"utf-8\") as f:\n",
    "                for url in f:\n",
    "                    encoded = urllib.parse.quote(url.encode(\"utf-8\"))\n",
    "                    fullEncode = \"https://m.facebook.com/l.php?u=\" + encoded\n",
    "            \n",
    "                    driver.get(fullEncode)\n",
    "                    driver.find_element_by_xpath(\"//a[@class='_42ft _42fu selected _42g-']\").click()\n",
    "                    time.sleep(20)\n",
    "                    driver.find_element_by_xpath(\"//div[@class='close-btn']\").click()\n",
    "                    source = driver.page_source\n",
    "                    soup = BeautifulSoup(source, 'lxml')\n",
    "                    story = soup.findAll('p')\n",
    "                    st = \"\"\n",
    "                    for x in story:\n",
    "                        st = st + str(x.get_text().replace('\\n', '').replace('\\r', ''))\n",
    "                    fin = [st]\n",
    "                    writer.writerow(fin)\n",
    "            #driver.close()\n",
    "        else:\n",
    "            writer = csv.writer(result, dialect='excel')\n",
    "            with open(articles[name][0], 'r', encoding=\"utf-8\") as f:\n",
    "                for url in f:\n",
    "                    #r = urllib.request.urlopen(url)\n",
    "                    #source = r.read()\n",
    "                    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    webpage = urlopen(req).read()\n",
    "                    soup = BeautifulSoup(webpage, 'lxml')\n",
    "                    if name == \"bbc\":\n",
    "                        storyDiv = soup.find('div', class_='story-body__inner')\n",
    "                        storyLis = storyDiv.find_all('p')\n",
    "                        st = \"\"\n",
    "                        for x in storyLis:\n",
    "                            st = st + str(x.get_text().replace('\\n', '').replace('\\r', ''))\n",
    "                        fin = [st]\n",
    "                        writer.writerow(fin)\n",
    "                    elif name == \"nyt\":\n",
    "                        matches = soup.findAll(classType(\"story-body-text story-content\"))\n",
    "                        st = \"\"\n",
    "                        for sect in matches:\n",
    "                            st = st + str(sect.get_text())\n",
    "                        fin = [st]\n",
    "                        writer.writerow(fin)\n",
    "                    elif name == \"an\":\n",
    "                        storyDiv = soup.find('div', class_='post-content clearfix')\n",
    "                        sup = storyDiv.find_all('p')\n",
    "                        st = \"\"\n",
    "                        for x in sup:\n",
    "                            st = st + str(x.get_text().replace('\\n', '').replace('\\r', ''))\n",
    "                        finish = [st]\n",
    "                        writer.writerow(finish)\n",
    "                    else:\n",
    "                        storyDiv = soup.find('div', class_='entry-content')\n",
    "                        storyLis = storyDiv.find_all('p')\n",
    "                        st = \"\"\n",
    "                        for x in storyLis:\n",
    "                            st = st + str(x.get_text().replace('\\n', '').replace('\\r', ''))\n",
    "                        st = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', st)\n",
    "                        st = re.sub(r'(\\d)([a-zA-Z])', r'\\1 \\2', st)\n",
    "                        st = re.sub(r\"[^'â€™0-9a-zA-Z#@, ]+\", ' ', st)\n",
    "                        finish = [st]\n",
    "                        writer.writerow(finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rt():\n",
    "    with open(\"rt.csv\", 'wb') as result:\n",
    "        writer = csv.writer(result, dialect='excel')\n",
    "        with open(\"rt.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "            for url in f:\n",
    "                r = urllib.request.urlopen(url).read()\n",
    "                soup = BeautifulSoup(r, 'lxml')\n",
    "                storyDiv = soup.find('div', class_='article__text text js-mediator-article')\n",
    "                storyLis = storyDiv.find_all('p')\n",
    "                st = \"\"\n",
    "                for x in storyLis:\n",
    "                    st = st + str(x.get_text())\n",
    "                finish = [st]\n",
    "                writer.writerow(finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Scrape('an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Search out tags for article content\n",
    "def classType(name):\n",
    "    #BS4 defines class tags as lists not strings\n",
    "    name = name.split()\n",
    "    def matches(minestrone):\n",
    "        classes = minestrone.get('class', '')\n",
    "        return all(c in classes for c in name)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scrape our New York Times articles\n",
    "def nytScrape():\n",
    "    with open(\"nyt.csv\", 'wb') as result:\n",
    "        writer = csv.writer(result, dialect='excel')\n",
    "        with open(fpath_nyt, 'r', encoding=\"utf-8\") as f:\n",
    "            for url in f:\n",
    "                r = urllib.request.urlopen(url).read()\n",
    "                soup = BeautifulSoup(r, 'lxml')\n",
    "                #matches = soup.findAll(class_=\"story-body-text story-content\")\n",
    "                matches = soup.findAll(classType(\"story-body-text story-content\"))\n",
    "                st = \"\"\n",
    "                for sect in matches:\n",
    "                    st = st + str(sect.get_text())\n",
    "                fin = [st]\n",
    "                writer.writerow(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scrape our Wall Street Journal articles\n",
    "def wsjScrape():\n",
    "    with open(\"wsj.csv\", 'wb') as result:\n",
    "        writer = csv.writer(result, dialect='excel')\n",
    "        driver = webdriver.Firefox()\n",
    "        with open(fpath_wsj, 'r', encoding=\"utf-8\") as f:\n",
    "            for url in f:\n",
    "                encoded = urllib.parse.quote(url.encode(\"utf-8\"))\n",
    "                fullEncode = \"https://m.facebook.com/l.php?u=\" + encoded\n",
    "                \n",
    "                driver.get(fullEncode)\n",
    "                driver.find_element_by_xpath(\"//a[@class='_42ft _42fu selected _42g-']\").click()\n",
    "                source = driver.page_source\n",
    "                soup = BeautifulSoup(source, 'lxml')\n",
    "                story = soup.findAll('p')\n",
    "                st = \"\"\n",
    "                for x in story[:-14]:\n",
    "                    st = st + str(x.get_text().replace('\\n', '').replace('\\r', ''))\n",
    "                fin = [st]\n",
    "                writer.writerow(fin)\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bbcScrape():\n",
    "    with open(\"bbc.csv\", 'wb') as result:\n",
    "        writer = csv.writer(result, dialect='excel')\n",
    "        with open(fpath_bbc, 'r', encoding=\"utf-8\") as f:\n",
    "            for url in f:\n",
    "                r = urllib.request.urlopen(url).read()\n",
    "                soup = BeautifulSoup(r, 'lxml')\n",
    "                storyDiv = soup.find('div', class_='story-body__inner')\n",
    "                storyLis = storyDiv.find_all('p')\n",
    "                st = \"\"\n",
    "                for x in storyLis:\n",
    "                    st = st + str(x.get_text().replace('\\n', '').replace('\\r', ''))\n",
    "                fin = [st]\n",
    "                writer.writerow(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def breitbartScrape():\n",
    "    with open(\"bb.csv\", 'wb') as result:\n",
    "        writer = csv.writer(result, dialect='excel')\n",
    "        with open(fpath_bb, 'r', encoding=\"utf-8\") as f:\n",
    "            for url in f:\n",
    "                r = urllib.request.urlopen(url).read()\n",
    "                soup = BeautifulSoup(r, 'lxml')\n",
    "                storyDiv = soup.find('div', class_='entry-content')\n",
    "                storyLis = storyDiv.find_all('p')\n",
    "                st = \"\"\n",
    "                for x in storyLis:\n",
    "                    st = st + str(x.get_text().replace('\\n', '').replace('\\r', ''))\n",
    "                \n",
    "                st = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', st)\n",
    "                st = re.sub(r'(\\d)([a-zA-Z])', r'\\1 \\2', st)\n",
    "                st = re.sub(r\"[^'â€™0-9a-zA-Z#@, ]+\", ' ', st)\n",
    "                finish = [st]\n",
    "                writer.writerow(finish)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onionScrape():\n",
    "    with open(\"theonionlinks.csv\", 'wb') as result:\n",
    "        writer = csv.writer(result, dialect='excel')\n",
    "        with open(fpath_onion, 'r', encoding=\"utf-8\") as f:\n",
    "            for url in f:\n",
    "                r = urllib.request.urlopen(url).read()\n",
    "                soup = BeautifulSoup(r, 'lxml')\n",
    "                matches = soup.findAll(class_=\"content-main-body\")\n",
    "                text = \"\"\n",
    "                for x in matches:\n",
    "                    text = text + str(x.get_text()).replace('\\n', '').replace('\\r', '')\n",
    "                finish = [text]\n",
    "                print(finish)\n",
    "                writer.writerow(finish)\n",
    "    result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breitbartScrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Bob Corker, who helped President O give us the bad Iran Deal   couldnâ€™t get elected dog catcher in Tennessee, is now fighting Tax Cuts,  Trump wrote on Twitter Bob Corker, who helped President O give us the bad Iran Deal   couldn't get elected dog catcher in Tennessee, is now fighting Tax Cuts  Donald J  Trump  @realDonaldTrump  October 24, 2017 Trump has previously used the  dog catcher  attack on Hillary Clinton during the presidential race, on Sen  Marco Rubio during the Republican presidential primary, and on Mayor Michael Bloomberg of New York City He added that Corker decided not to run for re election after Trump refused to endorse him and had since become opposed to his proposals Bob Corker, who helped President O give us the bad Iran Deal   couldn't get elected dog catcher in Tennessee, is now fighting Tax Cuts  Donald J  Trump  @realDonaldTrump  October 24, 2017 Corker dropped out of the race in Tennesse  sic  when I refused to endorse him, and now is only negative on anything Trump,  he wrote   Look at his record Corker told ABC News anchor George Stephanopoulos Tuesday that he stood by the comments he made about the White Houseâ€™s becoming an  adult daycare center  under Trump Corker also described Trumpâ€™s upcoming visit to Capitol Hill as a  photo op  and urged him to let Senate Republicans be in charge of tax overhaul policy What I hope is going to happen is the president will leave this effort, if you will, to the tax writing committees, let them do their work and not begin taking things off the table that ought to be debated in these committees at the proper time,  he complained Corker also urged Trump to stay out of foreign policy, especially on Twitter Leave it to the professionals for a while,  he said Corker fired back on Twitter, calling Trump a liar Same untruths from an utterly untruthful president,  he wrote with the hashtag #AlertTheDaycareStaff Same untruths from an utterly untruthful president  #AlertTheDaycareStaff  Senator Bob Corker  @SenBobCorker  October 24, 2017 Trump responded that  sad  Corker was a  lightweight Isn't it sad that lightweight Senator Bob Corker, who couldn't get re elected in the Great State of Tennessee, will now fight Tax Cuts plus  Donald J  Trump  @realDonaldTrump  October 24, 2017 Corker continued responding to the president in an interview on CNN Corker just now   The President has great difficulty with the truth on many issues  \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"bb.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    counter = 0\n",
    "    for x in f:\n",
    "        if counter == 0:\n",
    "            print(x)\n",
    "            break\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
