{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Robocop N-Grams calculations\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wall street journal articles - real\n",
    "with codecs.open('wsj.csv','r',encoding='utf8') as f:\n",
    "    text = f.read()    \n",
    "largeSample = []\n",
    "test = text.split()\n",
    "\n",
    "for i in test:\n",
    "    i = re.sub(r\"http\\D+[0-9]+\",\"\", i)\n",
    "    i = re.sub('[^A-Za-z0-9]+', '', i)\n",
    "    largeSample.append(i)    \n",
    "\n",
    "# bb news articles - fake\n",
    "with codecs.open('bb.csv','r',encoding='utf8') as r:\n",
    "    fake = r.read()    \n",
    "fakeSample = []\n",
    "fakeN = fake.split()\n",
    "\n",
    "for i in fakeN:\n",
    "    i = re.sub(r\"http\\D+[0-9]+\",\"\", i)\n",
    "    i = re.sub('[^A-Za-z0-9]+', '', i)\n",
    "    fakeSample.append(i)\n",
    "    \n",
    "#### new york times articles\n",
    "with codecs.open('nyt.csv','r',encoding='utf8') as f:\n",
    "    text2 = f.read()    \n",
    "largeSample2 = []\n",
    "test2 = text2.split()\n",
    "\n",
    "for i in test2:\n",
    "    i = re.sub(r\"http\\D+[0-9]+\",\"\", i)\n",
    "    i = re.sub('[^A-Za-z0-9]+', '', i)\n",
    "    largeSample2.append(i)\n",
    "    \n",
    "#### bbc articles\n",
    "with codecs.open('wsj.csv','r',encoding='utf8') as f:\n",
    "    text3 = f.read()    \n",
    "largeSample3 = []\n",
    "test3 = text3.split()\n",
    "\n",
    "for i in test3:\n",
    "    i = re.sub(r\"http\\D+[0-9]+\",\"\", i)\n",
    "    i = re.sub('[^A-Za-z0-9]+', '', i)\n",
    "    largeSample3.append(i)   \n",
    "\n",
    "#### rt articles - fake\n",
    "with codecs.open('rt.csv','r',encoding='utf8') as r:\n",
    "    fake2 = r.read()    \n",
    "fakeSample2 = []\n",
    "fakeN2 = fake2.split()\n",
    "\n",
    "for i in fakeN2:\n",
    "    i = re.sub(r\"http\\D+[0-9]+\",\"\", i)\n",
    "    i = re.sub('[^A-Za-z0-9]+', '', i)\n",
    "    fakeSample2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for getting N-Grams, counting occurrences of same type of n-gram 1, 2, 3\n",
    "\n",
    "def getNGrams(words, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(words)-(n-1)):\n",
    "        ngrams.append(words[i:i+n])\n",
    "    return ngrams\n",
    "\n",
    "bgs = nltk.bigrams(largeSample)\n",
    "\n",
    "#print bgs\n",
    "#print len(triGramReal)\n",
    "#print triGramReal\n",
    "\n",
    "# fake news tri-grams\n",
    "triGramFake = getNGrams(fakeSample, 3)\n",
    "#print len(triGramFake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55890\n",
      "[((u'of', u'the'), 536), ((u'in', u'the'), 378), ((u'Mr.', u'Trump'), 283), ((u'to', u'the'), 269), ((u'with', u'the'), 201), ((u'on', u'the'), 167), ((u'for', u'the'), 162), ((u'White', u'House'), 152), ((u'Mr.', u'Trump\\u2019s'), 140), ((u'the', u'U.S.'), 134), ((u'in', u'a'), 131), ((u'and', u'the'), 119), ((u'that', u'the'), 119), ((u'at', u'the'), 119), ((u'from', u'the'), 109), ((u'according', u'to'), 104), ((u'to', u'be'), 99), ((u'the', u'White'), 96), ((u'the', u'president'), 93), ((u'as', u'a'), 93)]\n",
      "\n",
      "37297\n",
      "[((u'the', u'US'), 282), ((u'in', u'the'), 279), ((u'of', u'the'), 262), ((u'to', u'the'), 188), ((u'Mr', u'Trump'), 147), ((u'on', u'the'), 119), ((u'with', u'the'), 101), ((u'that', u'the'), 98), ((u'for', u'the'), 86), ((u'White', u'House'), 84), ((u'from', u'the'), 83), ((u'North', u'Korea'), 81), ((u'and', u'the'), 78), ((u'at', u'the'), 77), ((u'to', u'be'), 75), ((u'Donald', u'Trump'), 70), ((u'said', u'the'), 69), ((u'in', u'a'), 69), ((u'as', u'a'), 68), ((u'by', u'the'), 67)]\n",
      "\n",
      "60342\n",
      "[((u'of', u'the'), 645), ((u'in', u'the'), 407), ((u'to', u'the'), 255), ((u'on', u'the'), 249), ((u'that', u'the'), 223), ((u'for', u'the'), 190), ((u'at', u'the'), 146), ((u'Mr.', u'Trump'), 141), ((u'and', u'the'), 139), ((u'with', u'the'), 123), ((u'in', u'a'), 121), ((u'the', u'United'), 120), ((u'to', u'be'), 118), ((u'by', u'the'), 117), ((u'from', u'the'), 110), ((u'that', u'he'), 106), ((u'United', u'States'), 105), ((u'of', u'a'), 100), ((u'as', u'a'), 93), ((u'more', u'than'), 87)]\n",
      "\n",
      "fake news articles\n",
      "27169\n",
      "[((u'of', u'the'), 248), ((u'in', u'the'), 227), ((u'to', u'the'), 154), ((u'U', u'S'), 124), ((u'Breitbart', u'News'), 96), ((u'that', u'the'), 81), ((u'the', u'U'), 79), ((u'on', u'the'), 76), ((u'and', u'the'), 73), ((u'to', u'be'), 65), ((u'with', u'the'), 61), ((u'for', u'the'), 61), ((u'at', u'the'), 57), ((u'by', u'the'), 55), ((u'the', u'Senate'), 54), ((u'the', u'United'), 49), ((u'from', u'the'), 46), ((u'he', u'said'), 45), ((u'on', u'Twitter'), 45), ((u'United', u'States'), 45)]\n",
      "\n",
      "36646\n",
      "[((u'of', u'the'), 352), ((u'in', u'the'), 282), ((u'the', u'US'), 212), ((u'to', u'the'), 202), ((u'on', u'the'), 116), ((u'that', u'the'), 112), ((u'with', u'the'), 84), ((u'for', u'the'), 84), ((u'to', u'be'), 73), ((u'at', u'the'), 73), ((u'by', u'the'), 70), ((u'from', u'the'), 67), ((u'and', u'the'), 62), ((u'as', u'a'), 61), ((u'in', u'a'), 60), ((u'is', u'a'), 59), ((u'has', u'been'), 55), ((u'according', u'to'), 55), ((u'that', u'he'), 50), ((u'said', u'that'), 48), ((u'White', u'House'), 44), ((u'North', u'Korea'), 43), ((u'is', u'the'), 41), ((u'it', u'is'), 40), ((u'the', u'United'), 37), ((u'would', u'be'), 36), ((u'said', u'in'), 36), ((u'of', u'a'), 34), ((u'the', u'Trump'), 31), ((u'is', u'not'), 31), ((u'one', u'of'), 31), ((u'to', u'do'), 30), ((u'going', u'to'), 30), ((u'more', u'than'), 29), ((u'that', u'is'), 28), ((u'said', u'the'), 28), ((u'as', u'the'), 27), ((u'the', u'White'), 26), ((u'the', u'Russian'), 26), ((u'that', u'it'), 26), ((u'it', u'was'), 26), ((u'State', u'Department'), 26), ((u'the', u'same'), 26), ((u'have', u'been'), 25), ((u'the', u'House'), 25), ((u'United', u'States'), 25), ((u'North', u'Korean'), 25), ((u'well', u'as'), 24), ((u'about', u'the'), 23), ((u'the', u'State'), 23)]\n",
      "4130\n"
     ]
    }
   ],
   "source": [
    "with open(\"wsj.csv\", \"rU\") as f:\n",
    "    text = csv.reader(f)\n",
    "    for row in text:\n",
    "        row[0] = re.sub(r\"http\\D+[0-9]+\",\"\", row[0])\n",
    "        sixgrams = ngrams(row[0].decode('utf8').split(), 2)\n",
    "result = collections.Counter(sixgrams)\n",
    "#print result \n",
    "with open(\"bb.csv\", \"rU\") as m:\n",
    "    text = csv.reader(m)\n",
    "    for row in text:\n",
    "        row[0] = re.sub(r\"http\\D+[0-9]+\",\"\", row[0])\n",
    "        sixgrams2 = ngrams(row[0].decode('utf8').split(), 3)\n",
    "result2 = collections.Counter(sixgrams2)\n",
    "#print result2\n",
    "\n",
    "###########################################################\n",
    "with open(\"wsj.csv\", \"rU\") as f:\n",
    "    gramtest = ngrams(f.read().decode('utf8').split(), 2)\n",
    "test = collections.Counter(gramtest)\n",
    "print len(test)\n",
    "print test.most_common(20)\n",
    "\n",
    "print \"\"\n",
    "\n",
    "with open(\"bbc.csv\", \"rU\") as f:\n",
    "    gramtest = ngrams(f.read().decode('utf8').split(), 2)\n",
    "test3 = collections.Counter(gramtest)\n",
    "print len(test3)\n",
    "print test3.most_common(20)\n",
    "\n",
    "print \"\"\n",
    "\n",
    "with open(\"nyt.csv\", \"rU\") as f:\n",
    "    gramtest = ngrams(f.read().decode('utf8').split(), 2)\n",
    "test4 = collections.Counter(gramtest)\n",
    "print len(test4)\n",
    "print test4.most_common(20)\n",
    "\n",
    "print \"\"\n",
    "print \"fake news articles\"\n",
    "\n",
    "\n",
    "with open(\"bb.csv\", \"rU\") as f:\n",
    "    gramtest = ngrams(f.read().decode('utf8').split(), 2)\n",
    "test2 = collections.Counter(gramtest)\n",
    "print len(test2)\n",
    "print test2.most_common(20)\n",
    "\n",
    "print \"\"\n",
    "\n",
    "with open(\"rt.csv\", \"rU\") as f:\n",
    "    gramtest = ngrams(f.read().decode('utf8').split(), 2)\n",
    "test5 = collections.Counter(gramtest)\n",
    "print len(test5)\n",
    "print test5.most_common(20)\n",
    "\n",
    "with open(\"an.csv\", \"rU\") as f:\n",
    "    gramtest = ngrams(f.read().decode('utf8').split(), 2)\n",
    "test6 = collections.Counter(gramtest)\n",
    "print len(test6)\n",
    "print test6.most_common(20)\n",
    "\n",
    "count = 0\n",
    "for i in test3:\n",
    "    for k in test2:\n",
    "        #print i, k\n",
    "        if i == k:\n",
    "            #print i, k\n",
    "            count = count + 1\n",
    "\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
